# -*- coding: utf-8 -*-
"""seqeuenceBaselineModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IGxN-RAtgkjUvgZgIL9CzPRa-jr4Dpz4
"""

## Libraries
import requests
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from torch.nn.functional import pad
from torch.utils.data import DataLoader, Dataset, random_split
import torch.optim as optim
from sklearn.metrics import confusion_matrix
import seaborn as sns

## Import data
raw_url = 'https://raw.githubusercontent.com/schaferd/8700-Project-EGNNs-TF-Binding-Prediction/main/pwm_seq_data_checkpoint%20(4).json'
response = requests.get(raw_url)
data = response.json()
data_vals = data['data']

## Convert list into dictionary

# Get keys from list
dict_data = {key: [] for key in data_vals[0].keys()}

# Iterate through list to fill new dictionary
for dictionary in data_vals:
    for key, value in dictionary.items():
        dict_data[key].append(value)

## Data Filtering
## Distribution of sequences lengths

max_len = max(dict_data['sequence'], key = len)
min_len = min(dict_data['sequence'], key = len)
# print(len(max_len))
# print(len(min_len))

string_lens = [len(seq) for seq in dict_data['sequence']]

# Plot the histogram
plt.hist(string_lens, bins=20, edgecolor='black')
plt.title('Distribution of String Lengths')
plt.xlabel('Length')
plt.ylabel('Frequency')
plt.show()

# Quartiles
print(np.percentile(string_lens, [5, 95]))

## Filter aa sequences between 46 and 1102

# Initialize dictionary
filtered_dict = {key: [] for key in dict_data.keys()}

# Filter seq by length
filtered_sequences = [sequence for sequence in dict_data['sequence'] if 46 <= len(sequence) <= 1102]
len(filtered_sequences)

# Update dictionary
for i in range(len(dict_data['sequence'])):
    if dict_data['sequence'][i] in filtered_sequences:
        for key in filtered_dict.keys():
            filtered_dict[key].append(dict_data[key][i])
filtered_dict = {key: value for key, value in filtered_dict.items() if value}

print(filtered_dict.keys())

## Embedding sequences
# define list of all amino acid sequences
aa_sequences = filtered_dict['sequence']
# print((aa_sequences))

# define amino acids
amino_acids = 'ACDEFGHIKLMNPQRSTVWYX'

# map aa - +1 keeps 0 for padding
aa_to_idx = {aa: i+1 for i, aa in enumerate(amino_acids)}

def convert_vector(sequence):
  seq_len = len(sequence)
  num_aa = len(amino_acids)

  converted_tensor = []
  for idx, aa in enumerate(sequence):
    if aa in aa_to_idx.keys():
      col_num = aa_to_idx[aa]
    else:
      col_num = aa_to_idx['X']
    converted_tensor.append(col_num)
  return converted_tensor

converted_seqs = [convert_vector(seq) for seq in aa_sequences]

# pad sequences, max of all the lists
max_rows = max(len(seq) for seq in converted_seqs)

# padding mask: keeps track of what is padded v. not
padding_mask = [[1] * len(seq) + [0] * (max_rows - len(seq)) for seq in converted_seqs]
padded_embeds = [seq + [0] * (max_rows - len(seq)) for seq in converted_seqs]

# convert sequences into Pytorch tensors
seqs_tensor = torch.tensor(padded_embeds) # Concatenates a sequence of tensors along a new dimension

## Convert to AT and GC-rich -- SWAP TO WHATEVER WE ARE USING AS PREDICTED -- SHOULD BE DONE BEFORE THIS FILE

# Extract list of PWMs (dict)
pwms = [pwm for pwm in filtered_dict['pwm']]

# Convert to AT-rich v. GC-rich
GC_rich = []

for idx, pwm in enumerate(pwms):
  AT_sum = sum(pwm['G']) + sum(pwm['C'])
  GC_sum = sum(pwm['A']) + sum(pwm['T'])

  if GC_sum >= AT_sum:
    GC_rich.append(1)
  else:
    GC_rich.append(0)
filtered_dict['GC_rich'] = GC_rich

print(len(filtered_dict['GC_rich']))
print(len(seqs_tensor))

## DATA SPLIT - CHANGE METHOD
# sequence similarity
# species

# Split train and test
torch.manual_seed(10)

# X = Embedded sequences, y = 0 or 1
model_data = list(zip(padded_embeds, padding_mask, filtered_dict['GC_rich']))

train_size = int(0.8*len(model_data))
val_size = int(0.1*len(model_data))
test_size = len(model_data) - train_size - val_size

# Random split on data
train_data, val_data, test_data = random_split(model_data, [train_size, val_size, test_size])

## TRAIN SEQUENCE MODEL: RNN

seq, pad, labels = zip(*train_data)

# Define a custom dataset class for the sequences.
class SeqDataset(Dataset):
    def __init__(self, data):
        seq, pad, labels = zip(*data)
        self.data = torch.tensor(seq, dtype = torch.long)
        self.pad = torch.tensor(pad, dtype = torch.float)
        self.labels = torch.tensor(labels, dtype=torch.float)

    def __getitem__(self, idx):
        return {'data': self.data[idx], 'pad': self.pad[idx], 'label': self.labels[idx]}

    def __len__(self):
        return len(self.data)

# Create data loaders for the training and test data.
batch_size = 32
train_loader = DataLoader(SeqDataset(train_data), batch_size=batch_size, shuffle = True)
val_loader = DataLoader(SeqDataset(val_data), batch_size=batch_size, shuffle = False)
test_loader = DataLoader(SeqDataset(test_data), batch_size=batch_size, shuffle = False)

class RNN(torch.nn.Module):

    def __init__(self, in_dim, hidden_dim, out_dim, dropout):
        super().__init__()

        self.embedding = torch.nn.Embedding(in_dim, hidden_dim)
        self.lstm = torch.nn.LSTM(
            hidden_dim,
            hidden_dim,
            dropout=dropout,
            batch_first=True,
            bidirectional=True
        )
        self.fc = torch.nn.Linear(2 * hidden_dim, out_dim)

    def forward(self, batch): # batch is a dictionary
        data = batch["data"] #
        # print(data.shape)
        pad_mask = batch["pad"] # RNN needs to handle padding; 0 when ignore a pad/fake token and 1 when real token
        # print(pad_mask.shape)
        # ML think abt masking bc things have different shapes (molecules and proteins have different number of atoms and residues)
        max_len = data.shape[1] # diff for each element in the batch

        # COMPLETE HERE

        # First we embed each input token into a vector - transfer tokens to vectors; indices into vectors (bag of words)
        # Use the self.embedding layer
        # every word encoded indv, after rnn, every word is fcn of another word, what token means in context of another word
        emb = self.embedding(data) # tensor, batch x seq_len x dim (# features)

        # Compute lengths from padding mask & use the above
        # emb: b x s x d, pad: b x s <- add columnwise for length
        # Compute the actual sequence length for each sample in the batch:
        lengths = pad_mask.sum(dim=1)

        # In order to ignore padding we use
        # out is tensor with batch of sequences, which have been padded (data)
        out = torch.nn.utils.rnn.pack_padded_sequence(emb, lengths=lengths, batch_first=True, enforce_sorted=False) # converts padded sequences to packed sequences

        # Now we pass it to the LSTM (which outputs out, state). Ignore the state.
        out, _ = self.lstm(out)

        # Now we unpack, we use
        out = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=max_len)[0]

        # Compute the average vector for the sequence
        # Beware of padding! Use the mask! Note that you will need to
        # expand the mask to a 3D Tensor. You can do so with mask.unsqueeze(-1)
        # you will also need to unsqueeze the lengths when dividing by them to take the average

        # print(out.shape)  # Expected: [batch_size, sequence_length, embedding_dim]
        # print(pad_mask.unsqueeze(-1).shape)  # Expected: [batch_size, sequence_length, 1]

        summed = torch.sum(out * pad_mask.unsqueeze(-1), dim=1)
        out = summed / lengths.unsqueeze(-1).float()

        # out = (torch.sum(emb * pad_mask.unsqueeze(-1), dim = 0)) / lengths.unsqueeze(-1).float()

        # Finally apply the fc layer
        out = self.fc(out).squeeze(-1) # squeeze last dim (dummy dim -> 32 len vector)

        return out

    # Function to train model

def train_model(model, train_loader, optimizer):
  model.train()
  total_loss = 0
  total_correct = 0
  total_items = 0

  # iterate over batches in data loader
  for batch in train_loader:
    optimizer.zero_grad()
    # forward pass - compute model preds
    output = model(batch)
    loss_function = nn.BCEWithLogitsLoss() # binary cross-entropy loss fcn
    loss_value = loss_function(output, batch['label'])
    # backward pass - compute gradients of loss with respect to model params
    loss_value.backward()
    # update params
    optimizer.step()

    total_loss += loss_value
    total_correct += torch.sum(output.sigmoid().round() == batch['label'])
    total_items += batch['data'].shape[0]
    print(loss_value.item())
  return total_loss.item() / len(train_loader), (total_correct / total_items).item()

# add validation
def val_model(model, val_loader, optimizer):
  model.eval()
  val_loss = 0
  total_correct = 0
  total_items = 0

  with torch.no_grad():
      for input, label in val_loader:
          output = model(input)
          loss_function = nn.BCEWithLogitsLoss()
          loss_value = loss_function(output, label)
          val_loss += loss_value
          total_correct += torch.sum(output.sigmoid().round() == label) # rounding to get label isn't good
          total_items += batch['data'].shape[0]

  val_loss /= len(val_loader)
  accuracy = total_correct / total_items

  return val_loss.item(), accuracy.item()

# Function to test model
def test_model(model, test_loader):
    model.eval()
    test_loss = 0
    total_correct = 0
    total_items = 0

    with torch.no_grad():
        for input, label in test_loader:
            output = model(input)
            loss_function = nn.BCEWithLogitsLoss()
            loss_value = loss_function(output, label)
            test_loss += loss_value
            total_correct += torch.sum(output.sigmoid().round() == label) # rounding to get label isn't good
            total_items += batch['data'].shape[0]

    test_loss /= len(test_loader)
    accuracy = total_correct / total_items

    return test_loss.item(), accuracy.item()

# Initialize Model, Optimizer
in_dim = 22 # padding and X
hidden_dim = 64
out_dim = 1
dropout = 0.1

model = RNN(in_dim, hidden_dim, out_dim, dropout)
optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)

# for i in range, number of passes - do a round of training and then a round of evaluation
# add a validation set

num_epochs = 100
# Save best model
best_model = None
best_accuracy = 0.0

for epoch in range(num_epochs):
  print(f"Epoch {epoch + 1}/{num_epochs}")

  # Train the model and evaluate its performance on the test data.
  train_loss, train_acc = train_model(model, train_loader, optimizer)
  val_loss, val_acc = val_model(model, val_loader)

  if val_acc > best_accuracy:
    best_accuracy = val_acc
    best_model = model.state_dict().copy()


  # Print the training and test results.
  print(f"Train loss: {train_loss}\tTrain accuracy: {train_acc}")
  print(f"Test loss: {val_loss}\tTest accuracy: {val_acc}")

# Save the best model
torch.save(best_model, 'best_model.pth')

# Test Model
test_loss, test_acc = test_model(best_model, test_loader)

# Print final results after all epochs
print("Final Results:")
print(f"Test accuracy: {test_acc}")

# Confusion Matrix
all_label = []
all_predictions = []

with torch.no_grad():
  for input, label in test_loader:
    output = best_model(input)
    predictions = (output.sigmoid() >= 0.5).int()  # Convert logits to binary predictions
    all_labels.extend(label.cpu().numpy())
    all_predictions.extend(predictions.cpu().numpy())

conf_matrix = confusion_matrix(all_labels, all_predictions)
print("Confusion Matrix:")
print(conf_matrix)

# Plot the confusion matrix
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1'], yticklabels=['0', '1'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()